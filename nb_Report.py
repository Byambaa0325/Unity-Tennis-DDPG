
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/Report.ipynb
from unityagents import UnityEnvironment
import numpy as np

env = UnityEnvironment(file_name="/data/Tennis_Linux_NoVis/Tennis")

# get the default brain
brain_name = env.brain_names[0]
brain = env.brains[brain_name]

# reset the environment
env_info = env.reset(train_mode=True)[brain_name]

# number of agents
num_agents = len(env_info.agents)
print('Number of agents:', num_agents)

# size of each action
action_size = brain.vector_action_space_size
print('Size of each action:', action_size)

# examine the state space
states = env_info.vector_observations
state_size = states.shape
print('There are {} agents. Total states shape: {}'.format(states.shape[0], state_size))
print('The state for the first agent looks like:', states[0])

import torch
from collections import deque
from ddpg_agent import Agent

env_info = env.reset(train_mode=True)[brain_name]
action_size = brain.vector_action_space_size
states = env_info.vector_observations
state_size = states.shape[1]

def get_states(states):
    return states[:1], states[1:]

def get_actions(states, agent, add_noise):
    state_0, state_1 = get_states(states)
    action_agent_0 = agent.act(state_0, add_noise)
    action_agent_1 = agent.act(state_1, add_noise)
    return np.concatenate((action_agent_0, action_agent_1), axis = 0)

def ddpg(n_episodes=500, max_t=1000, start_steps = 10, learn_frequency = 20, learn_count = 10, random_seed = 1):
    """Deep Deterministic Policy Gradient (DDPG)

    Params
    ======
        n_episodes (int)      : maximum number of training episodes
        max_t (int)           : maximum number of timesteps per episode
        start_steps (int)     : number of starting steps actions are chosen randomly
        learn_frequency (int) : frequency of learning per timestep
        learn_count (int)     : number of learning steps to do at learning timestep
        random_seed (int)     : random seed for agent's weights
    """

    agent = Agent(state_size=state_size, action_size=action_size, random_seed=random_seed)   #Initialize the Agent

    avg_scores_episode = []                    # list containing scores from each episode
    avg_scores_moving = []                     # list containing avg scores from window at each episode
    scores_window = deque(maxlen=100)          # last 100 scores

    for i_episode in range(1, n_episodes+1):
        env_info = env.reset(train_mode=True)[brain_name]       # reset environment
        states = env_info.vector_observations                   # get current state for each agent
        scores = np.zeros(num_agents)                           # initialize score for each agent
        agent.reset()                                           # reset noise of the agent

        for t in range(max_t):
            #Randomly sample actions during the starting steps
            if i_episode <= start_steps:
                actions = np.random.randn(num_agents, action_size) # select an action randomly
                actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1
            else:
                actions = agent.act(states, add_noise=True)#get_actions(states, agent, True)     # select an action according to policy (for each agent)
            env_info = env.step(actions)[brain_name]            # send actions to environment
            next_states = env_info.vector_observations          # get next state (for each agent)
            rewards = env_info.rewards                          # get reward (for each agent)
            dones = env_info.local_done                         # see if episode has finished (for each agent)

            # for each agent's experience, save it and learn
            for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):
                if t % learn_frequency == 0: # Learn with frequency
                    agent.step(state, action, reward, next_state, done, learn = True, learn_count = learn_count)
                else:
                    agent.step(state, action, reward, next_state, done, learn = False) #just add, don't learn

            states = next_states

            scores += np.max(rewards)                           # add the rewards from the timestep to the scores
            if np.any(dones):                                   # finish episode if any agent has reached a terminal state
                break


        scores_window.append(np.max(scores))            # save the most recent score to scores window

        avg_scores_episode.append(np.max(scores))       # save the most recent score to avg_scores
        avg_scores_moving.append(np.mean(scores_window)) # save the most recent score window average to moving averages


        print('\rEpisode {}\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end="")
        if i_episode % 1 == 0: # Print every episode
            print('\rEpisode {}\tAverage Score: {:.2f} \t Current Score: {:.2f}'.format(i_episode, np.mean(scores_window), np.mean(scores)))

        #environment is solved
        if np.mean(scores_window)>=0.5:
            print('\nEnvironment solved in {:d} episodes!\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))
            torch.save(agent.actor_local.state_dict(), "checkpoint_actor.pth")        #Save actors' weights
            torch.save(agent.critic_local.state_dict(), "checkpoint_critic.pth")      #Save critics' weights
            break

    return avg_scores_episode, avg_scores_moving # Return average score of each episode and moving average at that time

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--n_episodes', type=int, default=2000)
    parser.add_argument('--max_t', type=int, default=1000)
    parser.add_argument('--start_steps', type=int, default=100)
    parser.add_argument('--learn_frequency', type=int, default=10)
    parser.add_argument('--learn_count', type=int, default=5)
    parser.add_argument('--random_seed', type=int, default=1)
    args = parser.parse_args()

    ddpg(n_episodes = args.n_episodes, max_t = args.max_t, start_steps = args.start_steps, \
         learn_frequency = args.learn_frequency, learn_count = args.learn_count, random_seed = args.random_seed)
env.close()
